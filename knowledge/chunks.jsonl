{"source": "actions.txt", "id": "actions.txt:0", "text": "\nLUMO supports three main action types: web_search, save_note, and execute_action.\nDestructive actions like save_note require explicit user confirmation.\nRead-only actions like web_search execute immediately without confirmation.\nActions are logged with arguments and results for debugging and learning.\nThe action executor validates inputs and handles errors gracefully.\n"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:0", "text": "LUMO AI - DECEMBER 2025 DECISIONS\n\n=== ARCHITECTURE SNAPSHOT (2025-12-18) ===\n\nCURRENT STACK:\n  - Speech-to-Text: Faster-Whisper (offline, fast)\n  - Wake Word: Text-based \"lumo\" detection (simple, effective)\n  - Voice Activity Detection: WebRTC VAD (efficient)\n  - LLM: Ollama + Llama 3.1 (local, no API keys)\n  - Vector DB: FAISS (fast semantic search)\n  - Embeddings: SentenceTransformer (lightweight)\n  - Memory: JSON files (simple, portable)\n  - Logging: Structured JSON (auditable)\n\nRATIONALE:\n "}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:1", "text": "Logging: Structured JSON (auditable)\n\nRATIONALE:\n  All components selected for:\n  1. Local-first (no cloud dependencies)\n  2. Simplicity (minimal setup)\n  3. Performance (fast responses)\n  4. Privacy (data stays local)\n  5. Cost (free, open-source)\n\n---\n\n=== DECISION: LLM INFERENCE ARCHITECTURE ===\n\nDECISION: Use RAG (Retrieval-Augmented Generation)\nDATE: 2025-12-18\nCONTEXT:\n  Base LLM (Llama 3.1 7B) has knowledge cutoff\n  User might ask about system-specific info\n  Need grounded answers (not ha"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:2", "text": "stem-specific info\n  Need grounded answers (not hallucinations)\n\nCHOSEN APPROACH: RAG\n  1. Retrieve relevant chunks from knowledge base\n  2. Insert as system context in prompt\n  3. LLM answers using provided context\n  4. Sources are traceable\n\nBENEFITS:\n  \u2713 Answers grounded in facts (no hallucination)\n  \u2713 Can update knowledge without retraining\n  \u2713 Fast (retrieval + inference in 2-5 seconds)\n  \u2713 Transparent (can see sources)\n  \u2713 Extensible (add new knowledge anytime)\n\nALTERNATIVES CONSIDERED:\n  "}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:3", "text": "ew knowledge anytime)\n\nALTERNATIVES CONSIDERED:\n  Option A: Fine-tune LLM on knowledge\n    - Requires GPU, hours of training\n    - Hard to update\n    - Decision: TOO EXPENSIVE\n  \n  Option B: No augmentation (vanilla LLM)\n    - Simple, no retrieval overhead\n    - LLM hallucinates more often\n    - Can't use external knowledge\n    - Decision: TOO RISKY (unreliable)\n  \n  Option C: External API (OpenAI, Anthropic)\n    - Better quality\n    - Requires API keys, payment\n    - Privacy concerns\n    - Clou"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:4", "text": "PI keys, payment\n    - Privacy concerns\n    - Cloud dependency\n    - Decision: CONTRADICTS LOCAL-FIRST\n\nDECISION: RAG wins (grounded, updatable, fast, local)\n\n---\n\n=== DECISION: VECTOR EMBEDDING MODEL ===\n\nDECISION: Use SentenceTransformer all-MiniLM-L6-v2\nDATE: 2025-12-18\nCONTEXT:\n  Need embeddings for semantic search\n  Must work offline (no API)\n  Must fit on consumer hardware\n\nCHOSEN MODEL: all-MiniLM-L6-v2\n  - 22 MB (downloads in seconds)\n  - 384 dimensions (good balance)\n  - Trained on sema"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:5", "text": " 384 dimensions (good balance)\n  - Trained on semantic similarity\n  - Fast inference (CPU compatible)\n  - Well-maintained (active community)\n\nALTERNATIVES CONSIDERED:\n  Option A: OpenAI embeddings\n    - Highest quality\n    - Requires API key, $$ per request\n    - Cloud dependency\n    - Privacy concerns\n    - Decision: TOO EXPENSIVE & NOT LOCAL\n  \n  Option B: BERT (larger model)\n    - Good quality\n    - Slower inference\n    - Larger memory footprint\n    - Overkill for current use case\n    - Decis"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:6", "text": "nt\n    - Overkill for current use case\n    - Decision: TOO HEAVY\n  \n  Option C: TF-Hub models\n    - Lightweight\n    - Less optimized for semantic search\n    - Smaller community support\n    - Decision: LESS MATURE\n\nDECISION: SentenceTransformer for simplicity + quality\n\n---\n\n=== DECISION: CONFIRMATION GATE IMPLEMENTATION ===\n\nDECISION: Require explicit approval for destructive actions\nDATE: 2025-12-18\nCONTEXT:\n  System can execute actions (save_note, delete, etc.)\n  Need to prevent accidental dat"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:7", "text": "te, delete, etc.)\n  Need to prevent accidental data loss\n  Need to maintain user control and trust\n\nCHOSEN APPROACH: CONFIRMATION GATE\n  1. LLM suggests action (save_note, etc.)\n  2. System displays action to user\n  3. User must explicitly confirm (say \"yes\" or \"approve\")\n  4. 10-second timeout (fail-safe: reject if no response)\n  5. Log all decisions (audit trail)\n\nAFFECTED ACTIONS:\n  Require confirmation: save_note, delete, execute_custom_action\n  Auto-execute: web_search, query_memory, query_"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:8", "text": "n\n  Auto-execute: web_search, query_memory, query_knowledge\n\nBENEFITS:\n  \u2713 Safety (prevents mistakes)\n  \u2713 Control (user decides)\n  \u2713 Transparency (user sees what's happening)\n  \u2713 Trust (system won't surprise user)\n  \u2713 Accountability (logged decisions)\n\nALTERNATIVES CONSIDERED:\n  Option A: Always confirm\n    - Safest\n    - Slow, annoying UX\n    - Decision: TOO SLOW\n  \n  Option B: Never confirm\n    - Fastest\n    - High risk of mistakes\n    - Loss of control\n    - Decision: TOO RISKY\n  \n  Option C:"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:9", "text": "f control\n    - Decision: TOO RISKY\n  \n  Option C: Only confirm destructive\n    - Current approach\n    - Balances speed and safety\n    - Decision: CHOSEN (sweet spot)\n\nDECISION: Confirmation gate for destructive actions only\n\n---\n\n=== DECISION: KNOWLEDGE BASE CHUNKING STRATEGY ===\n\nDECISION: 500 chars per chunk, 50-char overlap\nDATE: 2025-12-18\nCONTEXT:\n  Need to chunk large documents for semantic search\n  Chunks too small: lose context\n  Chunks too large: don't fit in context window\n\nCHOSEN STR"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:10", "text": "too large: don't fit in context window\n\nCHOSEN STRATEGY: 500 chars, 50-char overlap\n  - Each chunk: ~500 characters\n  - Overlap: ~50 characters between chunks\n  - Result: ~1000-2000 chunks from typical document\n  - Fits in LLM context window\n  - Good granularity for search\n\nEXAMPLES:\n  Document: 5000 chars\n  \u2192 Chunks: ~10 chunks (with overlap)\n  \u2192 Search retrieves top-3\n  \u2192 LLM gets ~1500 chars of context\n  \u2192 Fits comfortably in any model's context window\n\nALTERNATIVES CONSIDERED:\n  Option A: 10"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:11", "text": "xt window\n\nALTERNATIVES CONSIDERED:\n  Option A: 1000 chars per chunk\n    - More context per chunk\n    - Better for coherence\n    - Slower search, more noise\n    - Decision: HARDER TO SEARCH PRECISELY\n  \n  Option B: 300 chars per chunk\n    - More granular\n    - Faster search\n    - More chunks to retrieve\n    - Decision: MIGHT LOSE CONTEXT\n  \n  Option C: Sentence-based chunks\n    - Natural boundaries\n    - Variable sizes\n    - Hard to control uniformity\n    - Decision: LESS PREDICTABLE\n\nDECISION: "}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:12", "text": "rmity\n    - Decision: LESS PREDICTABLE\n\nDECISION: 500/50 is Goldilocks zone (not too hot, not too cold)\n\n---\n\n=== DECISION: MEMORY BACKEND ===\n\nDECISION: Use JSON files for memory/knowledge storage\nDATE: 2025-12-18\nCONTEXT:\n  Need persistent storage for conversations and knowledge\n  Could use SQL, document DB, or files\n  Current scale: <10k interactions\n\nCHOSEN APPROACH: JSON FILES\n  - conversation_history.json: Past interactions\n  - chunks.jsonl: Knowledge chunks (one per line)\n  - metadata.jso"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:13", "text": ": Knowledge chunks (one per line)\n  - metadata.jsonl: Chunk metadata\n\nBENEFITS:\n  \u2713 Simple (no database server)\n  \u2713 Human-readable (can inspect with text editor)\n  \u2713 Portable (copy file to backup)\n  \u2713 Version-controllable (git-friendly)\n  \u2713 Zero setup (works out of box)\n\nALTERNATIVES CONSIDERED:\n  Option A: PostgreSQL / MongoDB\n    - Scales to millions of records\n    - Rich querying\n    - Requires database server setup\n    - More complex\n    - Decision: OVERKILL FOR CURRENT SCALE\n  \n  Option B: "}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:14", "text": "cision: OVERKILL FOR CURRENT SCALE\n  \n  Option B: SQLite\n    - Good middle ground\n    - Still requires setup/maintenance\n    - Not ideal for vector operations\n    - Decision: OVER-ENGINEERED\n  \n  Option C: JSON files (current)\n    - Simple\n    - Sufficient for current scale\n    - Can migrate to DB later\n    - Decision: CHOSEN (perfect for MVP)\n\nMIGRATION PATH:\n  If scale reaches 100k+ interactions:\n  1. Create SQLite schema\n  2. Migrate JSON \u2192 SQLite\n  3. Update queries to use SQL\n  4. Backward "}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:15", "text": "Lite\n  3. Update queries to use SQL\n  4. Backward compatible (system still works)\n\nDECISION: JSON files until scale demands migration\n\n---\n\n=== DECISION: LOGGING & AUDIT TRAIL ===\n\nDECISION: Structured JSON logging with session IDs\nDATE: 2025-12-18\nCONTEXT:\n  Need audit trail (accountability, debugging)\n  Could use plain text, binary, or structured format\n  Users need to inspect logs easily\n\nCHOSEN APPROACH: STRUCTURED JSON\n  Format: One JSON object per line (JSONL)\n  Fields: timestamp, session_"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:16", "text": "ect per line (JSONL)\n  Fields: timestamp, session_id, event_type, action, result, etc.\n  Storage: Plain text files (human-readable)\n  Parsing: Easy with jq, Python, or any JSON tool\n\nEXAMPLE LOG ENTRY:\n  {\n    \"timestamp\": \"2025-12-18T14:23:45Z\",\n    \"session_id\": \"abc123def456\",\n    \"event_type\": \"action_execution\",\n    \"action\": \"save_note\",\n    \"parameters\": {\"content\": \"Meeting at 3pm\"},\n    \"user_decision\": \"confirmed\",\n    \"result\": \"success\",\n    \"duration_ms\": 245\n  }\n\nBENEFITS:\n  \u2713 Stru"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:17", "text": "s\",\n    \"duration_ms\": 245\n  }\n\nBENEFITS:\n  \u2713 Structured (machine-parseable)\n  \u2713 Human-readable (can view as text)\n  \u2713 Append-only (fast writes)\n  \u2713 Traceable (session ID links events)\n  \u2713 Debuggable (full context in each entry)\n\nALTERNATIVES CONSIDERED:\n  Option A: Plain text logs\n    - Human-readable\n    - Hard to parse programmatically\n    - Decision: NO STRUCTURE\n  \n  Option B: Binary logging\n    - Compact\n    - Hard to inspect without tools\n    - Decision: NOT TRANSPARENT\n  \n  Option C: Str"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:18", "text": "    - Decision: NOT TRANSPARENT\n  \n  Option C: Structured JSON (current)\n    - Best of both worlds\n    - Readable + parseable\n    - Decision: CHOSEN\n\nDECISION: JSONL format for logging\n\n---\n\n=== DECISION: OFFLINE-FIRST vs CLOUD-FIRST ===\n\nDECISION: Offline-first (no cloud APIs by default)\nDATE: 2025-12-18\nCONTEXT:\n  Could use cloud LLMs (OpenAI, Anthropic)\n  Could use cloud embeddings (OpenAI)\n  Could use cloud speech-to-text (Google, AWS)\n  Trade-off: Cloud = better quality, offline = better pr"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:19", "text": "e-off: Cloud = better quality, offline = better privacy\n\nCHOSEN APPROACH: OFFLINE-FIRST\n  - LLM: Local (Ollama + Llama 3.1)\n  - Embeddings: Local (SentenceTransformer)\n  - Speech-to-text: Local (Faster-Whisper)\n  - Web search: Allowed (necessary for current info)\n\nPHILOSOPHY:\n  Privacy by default\n  All user data stays on machine\n  No API keys required\n  Works without internet (mostly)\n  User has full control\n\nBENEFITS:\n  \u2713 Privacy (no data sent externally)\n  \u2713 Cost (zero per-API charges)\n  \u2713 Spe"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:20", "text": "xternally)\n  \u2713 Cost (zero per-API charges)\n  \u2713 Speed (no network latency)\n  \u2713 Autonomy (user owns system)\n  \u2713 Trust (transparent, auditable)\n\nTRADEOFF:\n  \u2717 Quality: Smaller models than cloud\n  \u2717 Currency: No real-time internet knowledge\n  \u2717 Complexity: Local setup required\n\nALTERNATIVES CONSIDERED:\n  Option A: Cloud-first\n    - Better quality\n    - Real-time knowledge\n    - Requires API keys & $$\n    - Privacy concerns\n    - Internet dependency\n    - Decision: CONTRADICTS PRIVACY GOAL\n  \n  Optio"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:21", "text": "   - Decision: CONTRADICTS PRIVACY GOAL\n  \n  Option B: Hybrid (local + cloud fallback)\n    - Complexity\n    - Some online data required\n    - More moving parts\n    - Decision: OVERENGINEERED FOR MVP\n  \n  Option C: Offline-first (current)\n    - Privacy, cost, simplicity\n    - Trade acceptable quality for principles\n    - Decision: CHOSEN (aligns with values)\n\nMIGRATION PATH:\n  If user wants cloud options:\n  1. Add config flag (ENABLE_CLOUD_APIS)\n  2. Support OpenAI embeddings as fallback\n  3. Sup"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:22", "text": " 2. Support OpenAI embeddings as fallback\n  3. Support cloud LLM as option\n  4. Keep offline as default\n  5. User chooses their preference\n\nDECISION: Offline-first, with future option for cloud\n\n---\n\n=== DECISION: DEPLOYMENT STRATEGY ===\n\nDECISION: Single-machine, no server (user installs locally)\nDATE: 2025-12-18\nCONTEXT:\n  Could build web server (SaaS)\n  Could deploy to cloud\n  Could distribute via app store\n  Current: User runs on their machine\n\nCHOSEN APPROACH: LOCAL INSTALLATION\n  User down"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:23", "text": "e\n\nCHOSEN APPROACH: LOCAL INSTALLATION\n  User downloads code\n  User runs: python run.py\n  System listens on their machine\n  Web UI optional (local only)\n\nBENEFITS:\n  \u2713 Privacy (no servers storing data)\n  \u2713 Control (user owns everything)\n  \u2713 Simple (no cloud infrastructure)\n  \u2713 Cheap (no hosting costs)\n\nTRADEOFF:\n  \u2717 Setup: User must install Python, dependencies\n  \u2717 Accessibility: No mobile app (yet)\n  \u2717 Sync: Can't access from multiple devices\n  \u2717 Support: Limited (self-support)\n\nFUTURE OPTIONS:"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:24", "text": "\u2717 Support: Limited (self-support)\n\nFUTURE OPTIONS:\n  1. Package as executable (.exe, .dmg)\n  2. Docker container\n  3. Multi-device sync (peer-to-peer)\n  4. Optional cloud backup (opt-in)\n\nDECISION: Local-first, packages/containers for distribution\n\n---\n\n=== SUMMARY OF PHILOSOPHY ===\n\nLUMO 2025-12-18 is designed around:\n1. PRIVACY: Data stays on user's machine\n2. SIMPLICITY: Minimal dependencies, clear architecture\n3. COST: Free, open-source, no APIs\n4. CONTROL: User owns and operates the system\n"}
{"source": "DECISIONS_2025_12.txt", "id": "DECISIONS_2025_12.txt:25", "text": "PIs\n4. CONTROL: User owns and operates the system\n5. TRANSPARENCY: Full audit trail, explainable decisions\n6. EXTENSIBILITY: Easy to add knowledge, actions, models\n\nThese principles guide all technical decisions.\nIf a choice violates these principles, reconsider it.\n"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:0", "text": "LUMO AI DESIGN DECISIONS\n\n=== KEY ARCHITECTURAL CHOICES ===\n\nDECISION 1: LOCAL-FIRST ARCHITECTURE\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: Keep all processing local (no cloud APIs)\nRationale:\n  \u2713 Privacy: user data never leaves machine\n  \u2713 Reliability: no internet dependency\n  \u2713 Cost: zero API charges\n  \u2713 Speed: lower latency (no network round-trip)\n  \u2717 Tradeoff: smaller models (Llama 3.1 7B vs GPT-4)\n\nAlternative: Cloud-based (OpenAI, Anthropic)\n  + Larger, more capable models\n  - Require"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:1", "text": "ropic)\n  + Larger, more capable models\n  - Requires API keys & payments\n  - Privacy concerns (data sent to servers)\n  - Dependent on internet connection\n\nDecision: LOCAL-FIRST wins for privacy, cost, simplicity\n\n---\n\nDECISION 2: OLLAMA FOR LLM BACKEND\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: Ollama (self-hosted LLM runner)\nRationale:\n  \u2713 Simple to setup (one command)\n  \u2713 Multiple model support\n  \u2713 No API keys required\n  \u2713 Runs efficiently on consumer hardware\n  \u2717 Tradeoff: slower than cloud (n"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:2", "text": "nsumer hardware\n  \u2717 Tradeoff: slower than cloud (no GPU by default)\n\nAlternative: LlamaCPP, Hugging Face Inference\n  + Similar local-first approach\n  - More setup complexity\n  - Harder to switch models\n\nDecision: OLLAMA for ease of use\n\n---\n\nDECISION 3: FAISS FOR VECTOR SEARCH\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: FAISS (Facebook AI Similarity Search)\nRationale:\n  \u2713 Fast semantic search (orders of magnitude faster than brute-force)\n  \u2713 Lightweight (no separate database server needed)\n  \u2713 "}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:3", "text": "htweight (no separate database server needed)\n  \u2713 Battle-tested at scale (Meta uses it)\n  \u2713 Simple API\n  \u2717 Tradeoff: in-memory index (must reload on restart)\n\nAlternative: Pinecone, Weaviate, Milvus\n  + Full-featured vector DBs\n  - Require separate server/network\n  - More setup complexity\n  - Additional cost\n\nAlternative: SQLite + embedding column\n  + Simpler\n  - Much slower for semantic search\n  - Not designed for vectors\n\nDecision: FAISS for speed + simplicity\n\n---\n\nDECISION 4: SENTENCE-TRANSF"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:4", "text": "eed + simplicity\n\n---\n\nDECISION 4: SENTENCE-TRANSFORMERS FOR EMBEDDINGS\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: all-MiniLM-L6-v2 (SentenceTransformer model)\nRationale:\n  \u2713 Lightweight (22 MB, runs on CPU)\n  \u2713 High-quality embeddings (trained on semantic similarity)\n  \u2713 384-dimensional vectors (good balance of speed/quality)\n  \u2713 No API required (local model)\n  \u2717 Tradeoff: slower than larger models (OpenAI embeddings)\n\nAlternative: OpenAI embeddings\n  + State-of-the-art quality\n "}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:5", "text": ": OpenAI embeddings\n  + State-of-the-art quality\n  - Requires API key & payment\n  - Data sent to OpenAI servers\n  - Network dependency\n\nAlternative: TF-Hub models\n  + Lightweight\n  - Less well-optimized for semantic search\n\nDecision: SentenceTransformer for local, open-source simplicity\n\n---\n\nDECISION 5: CHUNKING STRATEGY (500 chars, 50-char overlap)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: Split documents into 500-char chunks with 50-char overlap\nRationale:\n  \u2713 500 cha"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:6", "text": "chunks with 50-char overlap\nRationale:\n  \u2713 500 chars fits within LLM context window (even small models)\n  \u2713 50-char overlap prevents losing context at chunk boundaries\n  \u2713 Results in ~1000-2000 chunks for reasonable knowledge base\n  \u2713 Balances speed (smaller chunks) vs completeness (larger chunks)\n  \u2717 Tradeoff: less context per chunk means more retrieval latency\n\nAlternatives:\n  - 1000 chars: less granular, more context per chunk, slower search\n  - 300 chars: more granular, faster search, less c"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:7", "text": " - 300 chars: more granular, faster search, less context\n  - 100 chars: very granular, very fast, but loses important info\n\nDecision: 500/50 is the \"Goldilocks zone\"\n\n---\n\nDECISION 6: K=3 FOR KNOWLEDGE RETRIEVAL\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: Retrieve top-3 chunks from knowledge base\nRationale:\n  \u2713 Enough context to answer most queries\n  \u2713 Fits in LLM context window (~1500 tokens)\n  \u2713 Reduces noise (top-5 or top-10 might include irrelevant chunks)\n  \u2713 Reasonable latency (FAISS "}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:8", "text": " irrelevant chunks)\n  \u2713 Reasonable latency (FAISS search is very fast)\n  \u2717 Tradeoff: less context = sometimes miss relevant info\n\nAlternative: K=1 (most relevant only)\n  + Very fast, minimal context\n  - High chance of missing relevant information\n\nAlternative: K=10 (comprehensive)\n  + Better coverage\n  - More noise, larger context window, slower\n\nDecision: K=3 balances relevance and completeness\n\n---\n\nDECISION 7: CONFIRMATION GATE FOR DESTRUCTIVE ACTIONS\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:9", "text": " ACTIONS\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: Require explicit user approval for save_note, delete, etc.\nRationale:\n  \u2713 Prevents accidental data loss\n  \u2713 User maintains control\n  \u2713 Safety-first design\n  \u2713 Builds trust (user knows system won't surprise them)\n  \u2717 Tradeoff: slows down workflow (user must confirm each action)\n\nAlternative: Auto-confirm all actions\n  + Faster workflow\n  - Higher risk of mistakes\n  - User loses control\n\nAlternative: Only confirm deletes\n  + C"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:10", "text": "s control\n\nAlternative: Only confirm deletes\n  + Compromise approach\n  - Inconsistent UX\n\nDecision: Confirm DESTRUCTIVE actions (write/delete), auto-execute READ-ONLY (search/query)\n\n---\n\nDECISION 8: 10-SECOND CONFIRMATION TIMEOUT\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: If user doesn't confirm within 10 seconds, reject action\nRationale:\n  \u2713 10 seconds is enough time for user to respond\n  \u2713 Prevents accidental auto-confirmation if user ignores\n  \u2713 Default-deny is safer than default-a"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:11", "text": "r ignores\n  \u2713 Default-deny is safer than default-allow\n  \u2717 Tradeoff: user must act quickly (might be annoying if AFK)\n\nAlternative: No timeout\n  + User can take their time\n  - System stalls waiting for response\n  - Might accidentally execute if user forgets\n\nAlternative: 30 seconds\n  + More time\n  - Too much time, feels slow\n\nDecision: 10 seconds is reasonable for voice interaction\n\n---\n\nDECISION 9: JSON LOGGING FORMAT\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: Store logs in structured JSON, one o"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:12", "text": "\u2500\u2500\u2500\u2500\u2500\nChoice: Store logs in structured JSON, one object per line (JSONL)\nRationale:\n  \u2713 Machine-parseable (can analyze programmatically)\n  \u2713 Structured (fields are defined)\n  \u2713 Human-readable (can view with jq or any JSON tool)\n  \u2713 Append-only (fast, doesn't require locking)\n  \u2717 Tradeoff: larger file size than binary format\n\nAlternative: Plain text logs\n  + Smaller files\n  - Hard to parse, no structure\n\nAlternative: SQLite database\n  + Rich querying\n  - More complex to setup\n  - Not needed for c"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:13", "text": "ing\n  - More complex to setup\n  - Not needed for current use case\n\nDecision: JSONL for simplicity + queryability\n\n---\n\nDECISION 10: LLAMA 3.1 MODEL CHOICE\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: Llama 3.1 (7B or 70B, user chooses)\nRationale:\n  \u2713 Open-source (no licensing issues)\n  \u2713 High quality (matches GPT-3.5 on many benchmarks)\n  \u2713 7B fits on consumer GPUs/CPUs\n  \u2713 70B available for power users\n  \u2717 Tradeoff: not quite as good as GPT-4\n\nAlternative: GPT-3.5/GPT-4\n  + Higher quality\n  - R"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:14", "text": "lternative: GPT-3.5/GPT-4\n  + Higher quality\n  - Requires API key & payment\n  - Privacy concerns\n\nAlternative: Mistral, Zephyr\n  + Good quality\n  - Less stable community/updates\n\nDecision: Llama 3.1 for quality + open-source\n\n---\n\nDECISION 11: NO PERSISTENT DATABASE, USE JSON FILES\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: Store interaction history and notes in JSON files\nRationale:\n  \u2713 Simple (no database server to maintain)\n  \u2713 Human-readable (can inspect with text editor)\n "}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:15", "text": " \u2713 Human-readable (can inspect with text editor)\n  \u2713 Portable (backup = copy a file)\n  \u2713 Version-controllable (can use git)\n  \u2717 Tradeoff: doesn't scale to millions of records\n\nAlternative: PostgreSQL, MongoDB\n  + Better for large scale\n  - Requires database setup/maintenance\n  - More complexity\n\nAlternative: SQLite\n  + Good middle ground\n  - Overkill for current use case\n\nDecision: JSON files until we need to scale\n\n---\n\nDECISION 12: MODULAR, NOT MONOLITHIC\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:16", "text": " MONOLITHIC\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: Organize code into independent modules (audio/, core/, learning/)\nRationale:\n  \u2713 Easy to test individual components\n  \u2713 Easy to replace/upgrade one component\n  \u2713 Clear separation of concerns\n  \u2713 New contributors can understand faster\n  \u2717 Tradeoff: slightly more complex than monolithic\n\nAlternative: Single app.py with everything\n  + Simpler to start\n  - Harder to maintain long-term\n  - Hard to test/debug\n  - Hard to extend\n\nDecision: MODUL"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:17", "text": " to test/debug\n  - Hard to extend\n\nDecision: MODULAR architecture for maintainability\n\n---\n\nDECISION 13: RAG (RETRIEVAL-AUGMENTED GENERATION)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nChoice: Inject retrieved knowledge into LLM prompt before inference\nRationale:\n  \u2713 LLM answers are grounded in facts (from knowledge base)\n  \u2713 Reduces hallucinations (made-up answers)\n  \u2713 Answers are traceable to source\n  \u2713 Knowledge base is easy to update\n  \u2717 Tradeoff: added latency (retrieval step before"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:18", "text": "  \u2717 Tradeoff: added latency (retrieval step before inference)\n\nAlternative: Fine-tune LLM on knowledge\n  + Answers baked in\n  - Requires retraining (expensive)\n  - Hard to update knowledge\n\nAlternative: No context (vanilla LLM)\n  + Simpler\n  - LLM hallucineates more often\n  - Can't use external knowledge\n\nDecision: RAG for grounded, updatable, traceable answers\n\n---\n\n=== DESIGN PRINCIPLES SUMMARY ===\n\n1. LOCAL-FIRST: No cloud, no APIs, privacy by default\n2. SIMPLE: Minimal dependencies, clear co"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:19", "text": " default\n2. SIMPLE: Minimal dependencies, clear code, easy to understand\n3. MODULAR: Independent components, easy to test/replace\n4. TRANSPARENT: Everything logged, full audit trail\n5. SAFE: Confirmation gates, input validation, error handling\n6. EXTENSIBLE: Easy to add actions, knowledge, models\n7. LEARNABLE: System improves from interactions\n8. MAINTAINABLE: Good documentation, clear interfaces\n\nThese principles guide all technical decisions.\nIf a choice violates these principles, reconsider i"}
{"source": "DESIGN_DECISIONS.txt", "id": "DESIGN_DECISIONS.txt:20", "text": "f a choice violates these principles, reconsider it.\n"}
{"source": "learning.txt", "id": "learning.txt:0", "text": "\nThe learning module tracks interaction patterns and provides improvement suggestions.\nIt analyzes wake word detection, success rates, and identifies failure patterns.\nLearning data is stored in learning logs for continuous improvement.\nThe system can auto-tune thresholds based on historical performance.\nLearning insights help LUMO become smarter over time.\n"}
{"source": "memory.txt", "id": "memory.txt:0", "text": "\nThe LUMO memory system uses FAISS (Facebook AI Similarity Search) for vector embeddings.\nIt stores conversation history in a FAISS index for fast semantic retrieval.\nMemory embeddings are 768-dimensional vectors from sentence transformers.\nYou can search past conversations by semantic similarity, not just keywords.\nMemory is persistent in memory/conversation_history.json.\n"}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:0", "text": "NOTES ON MEMORY SYSTEM\n\n=== FAISS VECTOR DATABASE ===\n\nWHAT IS FAISS?\n  FAISS = Facebook AI Similarity Search\n  A library for efficient similarity search and clustering of dense vectors\n  Designed for fast searching in high-dimensional spaces\n  Used at scale by Meta, Spotify, and other major companies\n\nWHY FAISS FOR MEMORY?\n  1. SPEED: Semantic search in milliseconds (not seconds)\n  2. SIMPLICITY: No separate database server needed\n  3. LOCAL: Runs entirely on user's machine\n  4. LIGHTWEIGHT: In"}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:1", "text": "ns entirely on user's machine\n  4. LIGHTWEIGHT: Index stored as single binary file\n  5. BATTLE-TESTED: Used at scale in production systems\n\nHOW IT WORKS:\n  1. Each chunk gets embedded as 384-dimensional vector\n  2. Vectors stored in FAISS index\n  3. When querying: compute L2 distance between query vector and all vectors\n  4. Return top-k (default: 3) nearest neighbors\n  5. Total time: <100ms for 10,000 chunks\n\n=== MEMORY FLOW ===\n\nSTORING:\n  User interaction \u2192 Embed with SentenceTransformer \u2192 Ad"}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:2", "text": " interaction \u2192 Embed with SentenceTransformer \u2192 Add to FAISS \u2192 Persist to disk\n\nQUERYING:\n  User query \u2192 Embed with SentenceTransformer \u2192 Search FAISS \u2192 Return chunks\n\nEXAMPLE:\n  User: \"How do I use notes?\"\n  Query embedding: [0.12, -0.45, 0.89, ...]  (384 dimensions)\n  FAISS search: Find 3 vectors closest to query embedding\n  Results:\n    Chunk 1: \"save_note action lets you persist...\"\n    Chunk 2: \"Notes are stored in JSON file...\"\n    Chunk 3: \"User confirmation required for save_note...\"\n\n=="}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:3", "text": " \"User confirmation required for save_note...\"\n\n=== MEMORY PERSISTENCE ===\n\nFILES:\n  - knowledge/index.faiss: Binary FAISS index (read-only after build)\n  - knowledge/metadata.jsonl: Chunk content + source (human-readable)\n  - memory/conversation_history.json: Interaction logs\n\nREBUILDING:\n  When you add new knowledge:\n  1. Run knowledge/ingest.py (chunks documents)\n  2. Run knowledge/index.py (builds new FAISS index)\n  3. Old index is overwritten\n  4. No backward compatibility needed (stateless"}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:4", "text": "n\n  4. No backward compatibility needed (stateless chunks)\n\n=== MEMORY LIMITATIONS & TRADEOFFS ===\n\nLIMITATION 1: IN-MEMORY ON STARTUP\n  Problem: Index loaded into RAM when system starts\n  Impact: Large knowledge bases (100k+ chunks) require more RAM\n  Solution: Could use FAISS GPU mode or disk-based alternatives\n\nLIMITATION 2: APPROXIMATE NEAREST NEIGHBORS\n  Problem: L2 distance doesn't always capture meaning perfectly\n  Example: \"cat\" and \"feline\" are semantically similar but different vectors"}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:5", "text": "ne\" are semantically similar but different vectors\n  Solution: Mitigated by using well-trained embeddings (SentenceTransformer)\n\nLIMITATION 3: STATIC INDEX\n  Problem: Can't add/remove chunks after building index\n  Impact: Requires full index rebuild when knowledge changes\n  Solution: Fast rebuild (milliseconds to seconds), acceptable for periodic updates\n\nLIMITATION 4: NO FILTERING\n  Problem: Can't filter results by date, source, or metadata\n  Example: Can't ask \"show me notes from last week\"\n  "}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:6", "text": "ample: Can't ask \"show me notes from last week\"\n  Solution: Could add metadata filtering in future\n\n=== OPTIMIZATION OPPORTUNITIES ===\n\nFUTURE 1: QUANTIZATION\n  Reduce vector dimensions (384 \u2192 128)\n  Faster search, less memory, slightly lower quality\n  Trade-off: speed vs recall\n\nFUTURE 2: INDEXING\n  Current: Linear search (IndexFlatL2)\n  Future: Use IVF (Inverted File) index for faster search on large datasets\n  Reduces search time from O(n) to O(k log n)\n\nFUTURE 3: DISK-BASED INDEX\n  Current: "}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:7", "text": "O(k log n)\n\nFUTURE 3: DISK-BASED INDEX\n  Current: In-memory (fast but RAM-heavy)\n  Future: Memory-mapped (scalable but slower)\n  Could support million-scale chunks\n\nFUTURE 4: METADATA FILTERING\n  Current: Return top-k chunks regardless of metadata\n  Future: Filter by source, date, type before returning\n  Example: \"Show notes from last 7 days about project X\"\n\n=== MEMORY BEST PRACTICES ===\n\n1. ORGANIZE KNOWLEDGE DOCUMENTS\n   Group related information in same .txt file\n   Helps FAISS find related "}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:8", "text": "ion in same .txt file\n   Helps FAISS find related chunks together\n   Example: \"project_guidelines.txt\" instead of scattered files\n\n2. CHUNK SIZE MATTERS\n   Current: 500 chars with 50-char overlap\n   Larger chunks (1000 chars): less retrieval calls but more noise\n   Smaller chunks (300 chars): more calls but cleaner results\n   Tune based on your content\n\n3. KEEP KNOWLEDGE FRESH\n   Update documents regularly\n   Rebuild index when significant changes\n   Old knowledge becomes stale over time\n\n4. MON"}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:9", "text": "s\n   Old knowledge becomes stale over time\n\n4. MONITOR QUALITY\n   Occasionally test: \"Can LUMO find the answer to X?\"\n   If not found: rephrase document or add keyword synonyms\n   Embeddings work on meaning, but explicit keywords help\n\n5. AVOID HUGE KNOWLEDGE BASES\n   Ingesting 100,000+ chunks takes minutes\n   Searching 100,000+ chunks takes <100ms (still fast)\n   But startup time and disk space grow\n   Recommendation: keep under 50,000 chunks per system\n\n=== MEMORY vs KNOWLEDGE BASE ===\n\nMEMORY"}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:10", "text": "r system\n\n=== MEMORY vs KNOWLEDGE BASE ===\n\nMEMORY (conversation history):\n  - What: Past user interactions\n  - Storage: memory/conversation_history.json\n  - Purpose: Give context to current query\n  - When queried: Return similar past interactions\n  - Grows over time: Yes (new interactions added each session)\n\nKNOWLEDGE BASE (external documents):\n  - What: Documentation, guides, facts\n  - Storage: knowledge/raw/*.txt + knowledge/index.faiss\n  - Purpose: Ground LLM answers in facts\n  - When queri"}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:11", "text": "urpose: Ground LLM answers in facts\n  - When queried: Return relevant chunks\n  - Grows over time: Only when explicitly added\n\nRELATIONSHIP:\n  Both use FAISS for semantic search\n  Both embedded with SentenceTransformer\n  Both return top-k results to LLM\n  Both are optional (system works without either)\n\nWHEN MEMORY IS MOST USEFUL:\n  - Follow-up questions: \"Tell me more about that\"\n  - Repeated requests: \"Remind me what we discussed\"\n  - Personalization: \"What did I ask you last week?\"\n\nWHEN KNOWL"}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:12", "text": "ation: \"What did I ask you last week?\"\n\nWHEN KNOWLEDGE BASE IS MOST USEFUL:\n  - Factual questions: \"How does LUMO work?\"\n  - External information: \"What's the weather?\"\n  - Configuration help: \"How do I set up X?\"\n\n=== DEBUGGING MEMORY ISSUES ===\n\nISSUE: Query returns irrelevant chunks\n  Cause: Vector embedding isn't matching semantically\n  Solution: Check SentenceTransformer model quality\n  Fix: Use larger model (all-mpnet-base-v2) if budget allows\n\nISSUE: Memory grows too large\n  Cause: Conver"}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:13", "text": "ows\n\nISSUE: Memory grows too large\n  Cause: Conversation history keeps growing\n  Solution: Implement history pruning (remove old interactions)\n  Fix: Keep only last N interactions (e.g., 1000)\n\nISSUE: Search is slow\n  Cause: FAISS index too large for available RAM\n  Solution: Reduce chunk count or use smaller embeddings\n  Fix: Use indexing (IVF) or disk-based approach\n\nISSUE: Same results for different queries\n  Cause: Embedding model isn't discriminative enough\n  Solution: Fine-tune embedding m"}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:14", "text": "iminative enough\n  Solution: Fine-tune embedding model on your domain\n  Fix: Use domain-specific embeddings (future work)\n\n=== MEMORY SECURITY & PRIVACY ===\n\nWHO CAN ACCESS MEMORY?\n  - Local: Only processes on same machine\n  - Network: Not accessible remotely (no server)\n  - Files: Readable by anyone with file access\n\nWHAT'S STORED?\n  - Transcriptions: Full text of user inputs\n  - Embeddings: Vector representations (non-reversible)\n  - Metadata: Source, timestamp, interaction details\n\nCAN IT BE "}
{"source": "NOTES_ON_MEMORY.txt", "id": "NOTES_ON_MEMORY.txt:15", "text": "Source, timestamp, interaction details\n\nCAN IT BE ENCRYPTED?\n  - Currently: No (future enhancement)\n  - Recommendation: Encrypt knowledge/index.faiss at rest\n  - Method: Use LUKS, BitLocker, or file-level encryption\n\nCAN USER DATA BE DELETED?\n  - Yes: Delete memory/conversation_history.json\n  - Yes: Rebuild knowledge/index.faiss without file\n  - Note: Embeddings can't be reversed to original text\n"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:0", "text": "LUMO AI PIPELINE FLOW - STEP BY STEP\n\n=== THE COMPLETE REQUEST-RESPONSE CYCLE ===\n\nPHASE 1: AUDIO CAPTURE & PROCESSING\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n1. LISTEN FOR WAKE WORD\n   - Microphone stream at 16kHz sample rate\n   - WebRTC VAD detects speech activity\n   - Look for \"lumo\" keyword in transcription\n   - Status: waiting for command\n\n2. CAPTURE VOICE INPUT\n   - Once wake word detected, record audio\n   - Silence detection: stop after 2+ seconds quiet\n   - Buffer audio chunks (typically 5"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:1", "text": "econds quiet\n   - Buffer audio chunks (typically 5-10 seconds)\n   - Timestamp for logging\n\n3. SPEECH-TO-TEXT (STT)\n   - Use Faster-Whisper model\n   - Convert audio \u2192 text transcription\n   - Extract duration in seconds\n   - Output: user_text, duration\n   Example: \"Lumo, what's the weather in New York?\"\n\n---\n\nPHASE 2: INTENT PARSING & RETRIEVAL\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n4. EXTRACT INTENT\n   - Strip wake word from transcription\n   - Normalize: lowercase, punctuation, extra spaces\n   - R"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:2", "text": "alize: lowercase, punctuation, extra spaces\n   - Result: \"what's the weather in new york?\"\n\n5. SEMANTIC MEMORY SEARCH\n   - Embed user query with SentenceTransformer\n   - Search FAISS index for similar past interactions\n   - Retrieve top-k (default: 3) chunks\n   - Add as context for LLM\n   Example: Find past weather queries\n\n6. KNOWLEDGE BASE RETRIEVAL (RAG)\n   - Embed same query\n   - Search knowledge/index.faiss\n   - Retrieve relevant documents\n   - Build context block from chunks\n   Example: \"W"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:3", "text": "  - Build context block from chunks\n   Example: \"Weather data shows typical patterns for...\"\n\n7. PREPARE MESSAGE HISTORY\n   - Start with system prompt (JARVIS personality)\n   - Add retrieved context (memory + knowledge)\n   - Add user query as latest message\n   - Include past turn history (optional)\n   Example:\n     System: \"You are LUMO...\"\n     System: \"[Knowledge: Weather patterns...]\"\n     User: \"what's the weather in new york?\"\n\n---\n\nPHASE 3: LLM INFERENCE\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n8. SEND TO "}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:4", "text": " LLM INFERENCE\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n8. SEND TO OLLAMA\n   - POST to http://localhost:11434/api/chat\n   - Model: Llama 3.1\n   - Messages: [system, context, user]\n   - Timeout: 60 seconds\n   - Stream: false (wait for full response)\n\n9. LLM PROCESSES\n   - Llama 3.1 reads context + query\n   - Generates response or action\n   - Output: text OR function call\n   Example output: \"The weather in New York is 45\u00b0F and cloudy\"\n   Example action: {\"action\": \"web_search\", \"query\": \"NYC weather\"}\n\n10. PARSE R"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:5", "text": "\"web_search\", \"query\": \"NYC weather\"}\n\n10. PARSE RESPONSE\n    - Check if text or function call\n    - If text: prepare for TTS output\n    - If action: extract action name + parameters\n    - Log for debugging\n\n---\n\nPHASE 4: ACTION EXECUTION (IF NEEDED)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n11. CHECK ACTION TYPE\n    - READ-ONLY (web_search): Execute immediately\n    - DESTRUCTIVE (save_note): Require confirmation\n    - Custom: Check in planner rules\n\n12. CONFIRMATION GATE (if needed)\n    - Display "}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:6", "text": "\n\n12. CONFIRMATION GATE (if needed)\n    - Display action to user\n    - Set 10-second timeout\n    - Wait for user approval\n    - Status: confirmed or rejected\n\n13. EXECUTE ACTION\n    - web_search: Query search engine\n    - save_note: Write to persistent storage\n    - execute_action: Call registered handler\n    - Catch exceptions, return result\n\n14. LOG ACTION\n    - Action name\n    - Parameters (args)\n    - Result (success/failure)\n    - Duration in milliseconds\n    - Log entry to core/logger.py\n\n"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:7", "text": "n milliseconds\n    - Log entry to core/logger.py\n\n---\n\nPHASE 5: RESPONSE GENERATION & OUTPUT\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n15. PREPARE FINAL RESPONSE\n    - If LLM gave text: use directly\n    - If action executed: \"I saved your note\" or \"Found X results\"\n    - Include action results if relevant\n    - Keep concise (JARVIS style)\n\n16. TEXT-TO-SPEECH (TTS)\n    - Use Piper TTS (optional, if configured)\n    - Convert response text \u2192 audio\n    - Play through speakers\n    - Log TTS execution\n\n"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:8", "text": " - Play through speakers\n    - Log TTS execution\n\n17. DISPLAY/LOG\n    - Print to console\n    - Save to session log\n    - Store in memory (for future context)\n    - Timestamp: full cycle time\n\n---\n\nPHASE 6: LEARNING & FEEDBACK\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n18. TRACK METRICS\n    - Wake word: detected? (yes/no)\n    - STT: confidence score\n    - LLM: response latency\n    - Action: success/failure\n    - User: satisfied? (implicit from next query)\n\n19. UPDATE LEARNING SYSTEM\n    - Log to learning/conv"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:9", "text": " UPDATE LEARNING SYSTEM\n    - Log to learning/conversation_history.json\n    - Analyze patterns:\n      - What actions fail most?\n      - What queries return best results?\n      - When is wake word missed?\n    - Suggest improvements\n\n20. AUTO-TUNE (optional)\n    - Adjust VAD thresholds if needed\n    - Modify wake word sensitivity\n    - Change RAG context size if helpful\n    - Adjust LLM temperature/parameters\n\n21. STORE FOR FUTURE\n    - Add interaction to memory (FAISS)\n    - Persist in JSON stora"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:10", "text": "tion to memory (FAISS)\n    - Persist in JSON storage\n    - Next time similar query comes: will retrieve this\n\n---\n\n=== EXAMPLE WALKTHROUGH ===\n\nUSER: \"Lumo, save a note about the FAISS database\"\n\nFLOW:\n  1. [AUDIO] Capture \"lumo, save a note about the faiss database\"\n  2. [STT] Transcribe to text\n  3. [INTENT] Extract: \"save a note about the faiss database\"\n  4. [MEMORY] Search past notes \u2192 find similar topics\n  5. [KNOWLEDGE] Retrieve docs about FAISS \u2192 load context\n  6. [LLM] Ollama sees: [sys"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:11", "text": " FAISS \u2192 load context\n  6. [LLM] Ollama sees: [system] [memory] [knowledge] [user query]\n  7. [OUTPUT] Llama responds: {\"action\": \"save_note\", \"content\": \"...\"}\n  8. [GATE] Ask confirmation: \"Save this note? (Y/n)\"\n  9. [EXECUTE] User confirms \u2192 write to storage\n  10. [LOG] Record: action=save_note, status=success, duration=245ms\n  11. [SPEAK] \"Note saved\" (via TTS)\n  12. [LEARN] Add interaction to history, update metrics\n  13. [NEXT] User's next similar query will find this note via memory sear"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:12", "text": " similar query will find this note via memory search\n\n---\n\n=== PERFORMANCE TARGETS ===\n\nWake Word Detection: <500ms\nSTT (transcription): 1-3 seconds\nMemory Search: <100ms\nLLM Inference: 2-10 seconds\nTotal Latency: 3-15 seconds (reasonable for voice AI)\n\n---\n\n=== KEY DESIGN DECISIONS ===\n\nCHUNKED KNOWLEDGE:\n  Documents split into 500-char chunks with overlap\n  Prevents \"context window overflow\"\n  Semantic search finds best chunks\n\nFAISS VECTOR SEARCH:\n  L2 distance metric for similarity\n  Fast (O"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:13", "text": "RCH:\n  L2 distance metric for similarity\n  Fast (O(log n) with indexing)\n  Scales to millions of chunks\n\nCONFIRMATION GATE:\n  Destructive operations blocked until approved\n  Safety mechanism for critical actions\n  Logged for accountability\n\nOFFLINE LLM:\n  No API calls, no rate limits\n  Privacy: all data stays local\n  Cost: zero\n  Trade-off: smaller model (Llama 3.1 7B)\n\nCONTINUOUS LEARNING:\n  Every interaction improves future performance\n  Thresholds auto-adjust\n  New interactions added to memor"}
{"source": "PIPELINE_FLOW.txt", "id": "PIPELINE_FLOW.txt:14", "text": "olds auto-adjust\n  New interactions added to memory\n  System becomes \"smarter\" over time\n"}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:0", "text": "LUMO AI SAFETY GUARDRAILS\n\n=== CONFIRMATION GATE MECHANISM ===\n\nPURPOSE:\n  Prevent accidental or malicious execution of destructive actions\n  Give user explicit control over critical operations\n  Maintain audit trail of user decisions\n\nHOW IT WORKS:\n  1. LLM generates action suggestion (save_note, execute_action, etc.)\n  2. System checks: is this action destructive?\n  3. If YES: block execution, show to user, wait for approval\n  4. If NO: execute immediately (web_search, query, etc.)\n  5. Log us"}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:1", "text": " immediately (web_search, query, etc.)\n  5. Log user decision (approved/rejected) + timestamp\n\nDESTRUCTIVE ACTIONS (require confirmation):\n  - save_note: Persists data\n  - execute_action: Runs custom code/scripts\n  - delete_note: Removes data\n  - clear_history: Erases interaction logs\n\nREAD-ONLY ACTIONS (execute immediately):\n  - web_search: Query internet\n  - query_memory: Search past interactions\n  - query_knowledge: Search knowledge base\n  - list_notes: View existing data\n\nCONFIRMATION PARAME"}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:2", "text": "ist_notes: View existing data\n\nCONFIRMATION PARAMETERS:\n  - Timeout: 10 seconds\n  - Default behavior if no response: REJECT (fail safe)\n  - User says \"yes\", \"confirm\", \"approve\": execute\n  - User says \"no\", \"cancel\", \"reject\": abort\n  - Timeout without response: abort silently\n\n=== INPUT VALIDATION ===\n\nRULE: NONE SHALL PASS EMPTY INPUT\n  - Empty transcription: ignore (continue listening)\n  - None text: ignore (don't process null)\n  - Whitespace-only: treat as empty\n  - Very short (<3 chars): li"}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:3", "text": "only: treat as empty\n  - Very short (<3 chars): likely garbage, ignore\n\nRULE: SANITIZE USER INPUT\n  - Remove leading/trailing whitespace\n  - Normalize punctuation\n  - Strip wake word from beginning\n  - Convert to lowercase for intent matching\n  - Never execute raw input (always process through LLM)\n\nRULE: VALIDATE ACTION PARAMETERS\n  - web_search: validate query length (>0, <500 chars)\n  - save_note: validate content (>0, <10000 chars)\n  - execute_action: whitelist allowed actions only\n  - Rejec"}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:4", "text": "e_action: whitelist allowed actions only\n  - Reject: parameters with shell metacharacters\n\n=== EXCEPTION HANDLING ===\n\nRULE: CATCH ALL, INFORM USER\n  - Wrap all risky operations in try-except\n  - Never silently swallow exceptions\n  - Return meaningful error to user\n  - Log full traceback for debugging\n\nRULE: NETWORK FAILURES\n  - If web_search fails: inform user \"couldn't search\"\n  - If Ollama unavailable: inform \"LLM offline\"\n  - If memory unavailable: inform \"memory system down\"\n  - Retry once "}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:5", "text": "lable: inform \"memory system down\"\n  - Retry once with exponential backoff\n\nRULE: RESOURCE EXHAUSTION\n  - If CPU high: slow down FAISS searches\n  - If memory low: prune old interactions\n  - If disk full: inform user, stop saving\n  - Never crash on resource limits\n\n=== PRIVACY & DATA PROTECTION ===\n\nRULE: LOCAL-ONLY PROCESSING\n  - No data sent to cloud (unless explicit web_search)\n  - All processing on user's machine\n  - User owns all interaction history\n  - User can delete/export logs anytime\n\nR"}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:6", "text": "history\n  - User can delete/export logs anytime\n\nRULE: CONVERSATION PRIVACY\n  - Store conversations in local JSON only\n  - Encrypt at rest (optional future enhancement)\n  - Don't share with third parties\n  - Inform user what's being logged\n\nRULE: LEARNED PATTERNS\n  - Learning system analyzes interactions\n  - Results inform suggestions/tuning only\n  - No personal info sent externally\n  - User can opt-out of learning\n\n=== CONFIRMATION SCENARIOS ===\n\nSCENARIO 1: Save a Note\n  User: \"Lumo, save a no"}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:7", "text": "\n\nSCENARIO 1: Save a Note\n  User: \"Lumo, save a note about project XYZ\"\n  System: \"Save note: 'About project XYZ...'? (Y/n)\"\n  User: \"yes\"\n  System: [executes save_note, logs decision]\n\nSCENARIO 2: Timeout (User Doesn't Respond)\n  System: [waits 10 seconds]\n  [no response]\n  System: \"Cancelled (no response)\" [logs rejection]\n\nSCENARIO 3: User Rejects\n  User: \"Lumo, save that\"\n  System: \"Save...? (Y/n)\"\n  User: \"no\"\n  System: \"Cancelled\" [logs rejection]\n\nSCENARIO 4: Ambiguous Intent\n  User: \"Lum"}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:8", "text": "ection]\n\nSCENARIO 4: Ambiguous Intent\n  User: \"Lumo, do the thing\"\n  System: \"I didn't understand. Did you want to:\n           A) Search the web\n           B) Save a note\n           Or something else?\"\n  User: [clarifies]\n\n=== LOGGING FOR ACCOUNTABILITY ===\n\nRULE: EVERYTHING IS LOGGED\n  - Every user input (transcription + timestamp)\n  - Every action (name, parameters, result)\n  - Every decision (user approval/rejection)\n  - Every error (type, stack trace, recovery)\n  - Every performance metric ("}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:9", "text": "ck trace, recovery)\n  - Every performance metric (latency, success rate)\n\nRULE: LOG FORMAT\n  - Structured JSON (parseable, not plain text)\n  - Include session ID for tracking\n  - Include user ID (optional, for multi-user)\n  - Include source (audio, text, api, etc.)\n  - Include outcome (success, failure, timeout)\n\nRULE: LOG ACCESS\n  - User can read logs anytime\n  - User can export logs to backup\n  - User can anonymize logs (remove transcription)\n  - User can delete logs (with confirmation)\n  - Sy"}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:10", "text": " - User can delete logs (with confirmation)\n  - System never auto-deletes logs\n\nEXAMPLE LOG ENTRY:\n  {\n    \"timestamp\": \"2025-12-18T14:23:45Z\",\n    \"session_id\": \"abc123\",\n    \"event_type\": \"action_execution\",\n    \"action\": \"save_note\",\n    \"parameters\": {\"content\": \"Project deadline next week\"},\n    \"user_decision\": \"confirmed\",\n    \"result\": \"success\",\n    \"duration_ms\": 245,\n    \"error\": null\n  }\n\n=== INJECTION ATTACK PREVENTION ===\n\nRULE: NO SHELL INJECTION\n  - Never pass user input directly"}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:11", "text": "SHELL INJECTION\n  - Never pass user input directly to os.system()\n  - Use subprocess with argument list, not string\n  - Validate all file paths (no ../ allowed)\n  - Escape web search queries\n\nRULE: NO PROMPT INJECTION\n  - Don't allow user to modify system prompt\n  - Keep system prompt separate from user messages\n  - Validate LLM input for suspicious patterns\n  - Log any attempted manipulation\n\nRULE: NO SQL INJECTION\n  - Use parameterized queries (if DB used)\n  - Never concatenate user input into"}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:12", "text": "(if DB used)\n  - Never concatenate user input into queries\n  - Validate input types\n\n=== PERMISSION MODEL ===\n\nIMPLICIT PERMISSIONS (user can always do):\n  - Query memory\n  - Search knowledge base\n  - Ask questions\n  - Get help/documentation\n\nEXPLICIT PERMISSIONS (requires confirmation):\n  - Save data\n  - Delete data\n  - Execute custom actions\n  - Modify system settings\n\nFUTURE PERMISSIONS (if multi-user added):\n  - Admin: can delete any user's data\n  - User: can only access own data\n  - Guest: "}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:13", "text": "ata\n  - User: can only access own data\n  - Guest: read-only access\n\n=== INCIDENT RESPONSE ===\n\nIF SYSTEM BEHAVES UNEXPECTEDLY:\n  1. Stop execution (pause on error)\n  2. Log full context (input, state, error)\n  3. Inform user with error message\n  4. Suggest manual check (review logs)\n  5. Continue listening (don't hang)\n  6. Alert user to check logs (on next interaction)\n\nIF USER SUSPECTS ABUSE:\n  1. User can stop system immediately\n  2. User can review full audit log\n  3. User can isolate system"}
{"source": "SAFETY_GUARDRAILS.txt", "id": "SAFETY_GUARDRAILS.txt:14", "text": "review full audit log\n  3. User can isolate system (disconnect network)\n  4. User can restore from backup\n  5. User can report issues (GitHub/email)\n\nIF PERFORMANCE DEGRADES:\n  1. Log increased latency\n  2. Check system resources (CPU, memory, disk)\n  3. If resource-constrained: prune old data\n  4. If software bug: check error logs\n  5. Restart system if necessary\n"}
{"source": "SYSTEM_OVERVIEW.txt", "id": "SYSTEM_OVERVIEW.txt:0", "text": "LUMO AI SYSTEM OVERVIEW\n\n=== WHAT IS LUMO? ===\nLUMO is a JARVIS-style voice AI assistant that runs locally on your machine.\nIt combines speech recognition, language models, semantic memory, and action execution\ninto a cohesive intelligent agent.\n\n=== CORE CAPABILITIES ===\n\n1. VOICE INTERACTION\n   - Listens for wake word \"lumo\"\n   - Uses Faster-Whisper for offline speech-to-text\n   - VAD (voice activity detection) to handle silence\n   - Outputs transcribed text\n\n2. UNDERSTANDING\n   - Local LLM (O"}
{"source": "SYSTEM_OVERVIEW.txt", "id": "SYSTEM_OVERVIEW.txt:1", "text": "anscribed text\n\n2. UNDERSTANDING\n   - Local LLM (Ollama + Llama 3.1) for language understanding\n   - RAG (Retrieval-Augmented Generation) for knowledge context\n   - Semantic memory via FAISS vector search\n   - Intent detection and action routing\n\n3. ACTIONS\n   - Web search integration\n   - Note-taking and persistence\n   - Custom action execution\n   - Confirmation gates for destructive operations\n\n4. LEARNING\n   - Tracks interaction patterns\n   - Measures wake word detection accuracy\n   - Monitor"}
{"source": "SYSTEM_OVERVIEW.txt", "id": "SYSTEM_OVERVIEW.txt:2", "text": "Measures wake word detection accuracy\n   - Monitors action success rates\n   - Auto-tunes system thresholds\n   - Provides improvement suggestions\n\n5. LOGGING\n   - Session-based event tracking\n   - Comprehensive audit trail\n   - Structured JSON logging\n   - Performance metrics\n\n=== ARCHITECTURE LAYERS ===\n\nLAYER 1: INPUT (Audio/Text)\n\u251c\u2500 Microphone input\n\u251c\u2500 STT (speech-to-text)\n\u2514\u2500 Wake word detection\n\nLAYER 2: REASONING (LLM + Memory)\n\u251c\u2500 Vector semantic search (FAISS)\n\u251c\u2500 Knowledge retrieval (RAG)\n\u251c"}
{"source": "SYSTEM_OVERVIEW.txt", "id": "SYSTEM_OVERVIEW.txt:3", "text": "ntic search (FAISS)\n\u251c\u2500 Knowledge retrieval (RAG)\n\u251c\u2500 LLM inference (Ollama)\n\u2514\u2500 Intent detection\n\nLAYER 3: EXECUTION (Actions + Confirmation)\n\u251c\u2500 Action executor\n\u251c\u2500 Confirmation state machine\n\u2514\u2500 Error handling\n\nLAYER 4: FEEDBACK (Learning + Logging)\n\u251c\u2500 Event logging\n\u251c\u2500 Performance tracking\n\u251c\u2500 System tuning\n\u2514\u2500 Improvement analysis\n\n=== KEY TECHNICAL COMPONENTS ===\n\nMEMORY:\n  - Engine: FAISS vector database\n  - Embedding model: sentence-transformers (all-MiniLM-L6-v2)\n  - Storage: JSON file (persiste"}
{"source": "SYSTEM_OVERVIEW.txt", "id": "SYSTEM_OVERVIEW.txt:4", "text": "all-MiniLM-L6-v2)\n  - Storage: JSON file (persistent)\n  - Purpose: Find contextually relevant past interactions\n\nLLM:\n  - Backend: Ollama (local, no API keys)\n  - Model: Llama 3.1\n  - System prompt: JARVIS personality (concise, action-focused)\n  - RAG: Augment with knowledge base chunks\n\nACTIONS:\n  - web_search: Query the internet\n  - save_note: Persistent note storage\n  - execute_action: Extensible custom actions\n  - All actions are logged and can require confirmation\n\nLEARNING:\n  - Tracks succ"}
{"source": "SYSTEM_OVERVIEW.txt", "id": "SYSTEM_OVERVIEW.txt:5", "text": "an require confirmation\n\nLEARNING:\n  - Tracks success rates\n  - Monitors wake word accuracy\n  - Identifies patterns in failures\n  - Auto-tunes system parameters\n  - Generates improvement recommendations\n\n=== DATA FLOW ===\n\nUSER SPEAKS\n  \u2193\nTRANSCRIBE (STT)\n  \u2193\nPARSE INTENT\n  \u251c\u2500\u2192 Search past interactions (FAISS)\n  \u251c\u2500\u2192 Retrieve knowledge base (RAG)\n  \u2514\u2500\u2192 Infer with LLM (Ollama)\n  \u2193\nGENERATE RESPONSE\n  \u251c\u2500\u2192 Text (speak via TTS)\n  \u2514\u2500\u2192 Action (execute with confirmation if needed)\n  \u2193\nLOG & LEARN\n  \u251c\u2500\u2192 "}
{"source": "SYSTEM_OVERVIEW.txt", "id": "SYSTEM_OVERVIEW.txt:6", "text": "ith confirmation if needed)\n  \u2193\nLOG & LEARN\n  \u251c\u2500\u2192 Track success/failure\n  \u251c\u2500\u2192 Update statistics\n  \u2514\u2500\u2192 Improve for next time\n\n=== SYSTEM CHARACTERISTICS ===\n\nOFFLINE-FIRST:\n  No cloud dependencies. All processing local.\n\nTRANSPARENT:\n  Every action is logged. Full audit trail.\n\nMODULAR:\n  Components are independent and testable.\n\nADAPTIVE:\n  Learns from interactions. Improves over time.\n\nSAFE:\n  Destructive actions require explicit confirmation.\n\n=== GETTING STARTED ===\n\n1. Configure Ollama with "}
{"source": "SYSTEM_OVERVIEW.txt", "id": "SYSTEM_OVERVIEW.txt:7", "text": "=== GETTING STARTED ===\n\n1. Configure Ollama with Llama 3.1\n2. Add knowledge documents to knowledge/raw/\n3. Run: python knowledge/ingest.py && python knowledge/index.py\n4. Start LUMO: python run.py\n5. Speak: \"Lumo, <command>\"\n\n=== EXTENDING LUMO ===\n\nADD KNOWLEDGE:\n  1. Create .txt file in knowledge/raw/\n  2. Run knowledge/ingest.py to chunk\n  3. Run knowledge/index.py to index with FAISS\n\nADD ACTIONS:\n  1. Implement in core/planner.py or a new module\n  2. Update LLM system prompt if needed\n  3."}
{"source": "SYSTEM_OVERVIEW.txt", "id": "SYSTEM_OVERVIEW.txt:8", "text": "odule\n  2. Update LLM system prompt if needed\n  3. Add to logging in core/logger.py\n\nCUSTOMIZE LLM:\n  1. Modify SYSTEM_PROMPT in core/llm.py\n  2. Adjust model in Ollama settings\n  3. Tune RAG context size (k parameter)\n"}
{"source": "SYSTEM_RULES.txt", "id": "SYSTEM_RULES.txt:0", "text": "LUMO AI SYSTEM RULES\n\n=== CORE BEHAVIORAL RULES ===\n\nRULE 1: CONCISENESS\n  - Keep responses brief (under 100 words for voice)\n  - No unnecessary elaboration\n  - Direct, actionable answers\n  - Mimic JARVIS personality\n\nRULE 2: INTENT-FIRST DECISION MAKING\n  - Parse user intent before responding\n  - If intent is unclear, ask clarifying question\n  - If intent matches an action, execute (with confirmation if needed)\n  - If intent is pure information, use LLM + knowledge base\n\nRULE 3: ACTION EXECUTIO"}
{"source": "SYSTEM_RULES.txt", "id": "SYSTEM_RULES.txt:1", "text": " use LLM + knowledge base\n\nRULE 3: ACTION EXECUTION PRIORITY\n  - Read-only actions (web_search) execute immediately\n  - Destructive actions (save_note) require explicit confirmation\n  - All action results are logged\n  - If action fails, inform user and suggest alternative\n\nRULE 4: MEMORY & CONTEXT\n  - Always check memory (FAISS) for relevant past interactions\n  - Inject retrieved context into LLM prompt\n  - New interactions automatically added to memory\n  - Context improves future responses on s"}
{"source": "SYSTEM_RULES.txt", "id": "SYSTEM_RULES.txt:2", "text": " memory\n  - Context improves future responses on similar queries\n\nRULE 5: KNOWLEDGE BASE AUGMENTATION\n  - Retrieve 3 chunks from knowledge base for complex queries\n  - Prioritize relevant documents (FAISS distance < threshold)\n  - Include source in logs for traceability\n  - Update knowledge base regularly with new documents\n\nRULE 6: CONFIRMATION GATE\n  - Actions requiring confirmation: save_note, execute_action\n  - Timeout: 10 seconds\n  - If user doesn't respond: assume rejection\n  - Log user de"}
{"source": "SYSTEM_RULES.txt", "id": "SYSTEM_RULES.txt:3", "text": " doesn't respond: assume rejection\n  - Log user decision for learning\n\nRULE 7: SILENCE & INACTIVITY\n  - If no wake word: remain listening\n  - If transcription is empty: ignore\n  - If user doesn't speak within timeout: timeout gracefully\n  - Never act on ambiguous or null input\n\nRULE 8: ERROR HANDLING\n  - Catch all exceptions in action executors\n  - Return meaningful error to user\n  - Log full error trace for debugging\n  - Suggest fallback action if primary fails\n\nRULE 9: LOGGING & TRANSPARENCY\n "}
{"source": "SYSTEM_RULES.txt", "id": "SYSTEM_RULES.txt:4", "text": "if primary fails\n\nRULE 9: LOGGING & TRANSPARENCY\n  - Every action logged with timestamp\n  - Session ID for tracking related events\n  - Structured JSON for programmatic access\n  - User can review full audit trail\n\nRULE 10: LEARNING & ADAPTATION\n  - Track success/failure for each action\n  - Update metrics after each interaction\n  - Periodically auto-tune system parameters\n  - Suggest improvements to user when available\n\n=== INTERACTION RULES ===\n\nSTARTING A SESSION:\n  1. Initialize session logger "}
{"source": "SYSTEM_RULES.txt", "id": "SYSTEM_RULES.txt:5", "text": "TARTING A SESSION:\n  1. Initialize session logger with unique ID\n  2. Load memory (FAISS index)\n  3. Load knowledge base (if available)\n  4. Begin listening for wake word\n\nHANDLING USER INPUT:\n  1. Transcribe audio to text\n  2. Extract and normalize intent\n  3. Check confirmation cache (are we waiting for user response?)\n  4. Retrieve memory context (top-3 similar chunks)\n  5. Retrieve knowledge context (top-3 from knowledge base)\n  6. Pass to LLM with full context\n  7. Parse LLM response (text "}
{"source": "SYSTEM_RULES.txt", "id": "SYSTEM_RULES.txt:6", "text": "M with full context\n  7. Parse LLM response (text or action)\n  8. Execute or confirm as needed\n\nRESPONDING TO USER:\n  1. If text response: speak via TTS (if available) OR display\n  2. If action response: show confirmation, wait for approval\n  3. Once approved/rejected: log decision\n  4. Add interaction to memory for next time\n  5. Return to listening (go back to STARTING A SESSION)\n\nCLOSING A SESSION:\n  1. Save all logs to file\n  2. Update learning metrics\n  3. Persist memory to disk\n  4. Close "}
{"source": "SYSTEM_RULES.txt", "id": "SYSTEM_RULES.txt:7", "text": "ng metrics\n  3. Persist memory to disk\n  4. Close any open connections\n\n=== CONTENT RULES ===\n\nWHAT LUMO SHOULD DO:\n  \u2713 Answer factual questions\n  \u2713 Search the web for current information\n  \u2713 Save notes and reminders\n  \u2713 Execute configured actions\n  \u2713 Explain how the system works\n  \u2713 Learn from interactions\n  \u2713 Suggest improvements\n\nWHAT LUMO SHOULD NOT DO:\n  \u2717 Bypass confirmation gates\n  \u2717 Delete data without permission\n  \u2717 Make irreversible changes without logging\n  \u2717 Share sensitive informati"}
{"source": "SYSTEM_RULES.txt", "id": "SYSTEM_RULES.txt:8", "text": "nges without logging\n  \u2717 Share sensitive information publicly\n  \u2717 Act on null or ambiguous input\n  \u2717 Ignore error conditions\n  \u2717 Pretend to have capabilities it doesn't\n\n=== PERFORMANCE RULES ===\n\nRULE: RESPONSE LATENCY\n  - Target: <5 seconds for most queries\n  - Acceptable: 5-10 seconds\n  - Timeout: >15 seconds (inform user)\n\nRULE: MEMORY EFFICIENCY\n  - Limit FAISS search to top-3 results\n  - Limit knowledge base search to top-3 results\n  - Total context window: ~2000 tokens max\n  - Prune old i"}
{"source": "SYSTEM_RULES.txt", "id": "SYSTEM_RULES.txt:9", "text": "l context window: ~2000 tokens max\n  - Prune old interactions periodically\n\nRULE: ERROR RECOVERY\n  - Retry failed network requests once\n  - Fall back to local LLM if web search fails\n  - Continue listening even if action fails\n  - Never crash on edge case input\n\n=== EXTENSION RULES ===\n\nTO ADD A NEW ACTION:\n  1. Implement in core/planner.py or new module\n  2. Register with LLM system prompt (describe what it does)\n  3. Decide: requires confirmation? (default: yes for destructive)\n  4. Add loggin"}
{"source": "SYSTEM_RULES.txt", "id": "SYSTEM_RULES.txt:10", "text": "on? (default: yes for destructive)\n  4. Add logging to core/logger.py\n  5. Test with manual invocation\n  6. Add to learning system for tracking\n\nTO ADD KNOWLEDGE:\n  1. Create .txt file in knowledge/raw/\n  2. Run: python knowledge/ingest.py (chunk the text)\n  3. Run: python knowledge/index.py (build FAISS index)\n  4. Test: ask a query related to the knowledge\n  5. Verify: check that chunks are retrieved\n  6. Iterate: refine document structure if needed\n\nTO CUSTOMIZE BEHAVIOR:\n  1. Modify SYSTEM_P"}
{"source": "SYSTEM_RULES.txt", "id": "SYSTEM_RULES.txt:11", "text": "eeded\n\nTO CUSTOMIZE BEHAVIOR:\n  1. Modify SYSTEM_PROMPT in core/llm.py (for personality)\n  2. Adjust RAG k-parameter in ask_llm_with_knowledge() (for context size)\n  3. Change chunk size in knowledge/ingest.py (for granularity)\n  4. Update confirmation timeout in core/confirmation.py (for patience)\n  5. Test with sample interactions before deploying\n"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:0", "text": "WHY KEYWORD SPOTTING MATTERS\n\n=== THE WAKE WORD DETECTION PROBLEM ===\n\nTHE CHALLENGE:\n  A voice assistant must listen 24/7 to the environment\n  But you don't want it responding to every conversation\n  \"Alexa, can you help me?\" \u2192 \"Lumo, what's for dinner?\" \u2192 \"Lumo\" appears in TV dialogue\n  How do you know when the user is actually addressing YOU?\n\nTHE SOLUTION: WAKE WORD (KEYWORD SPOTTING)\n  Designate a unique wake word: \"Lumo\"\n  Device listens for this word specifically\n  Only processes audio af"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:1", "text": "r this word specifically\n  Only processes audio after wake word detected\n  Ignores background noise, other conversations, TV\n\n=== WHY \"LUMO\" SPECIFICALLY ===\n\nGOOD WAKE WORD CHARACTERISTICS:\n  \u2713 Unique phonetically (not common in English)\n  \u2713 Short (1 syllable = easy to trigger)\n  \u2713 Distinctive pronunciation (hard to confuse)\n  \u2713 Not offensive in any language\n  \u2713 Easy to remember\n\nWHY NOT \"COMPUTER\"?\n  \u2717 Too common (appears in conversation frequently)\n  \u2717 Two syllables (harder to trigger reliabl"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:2", "text": "ntly)\n  \u2717 Two syllables (harder to trigger reliably)\n  \u2717 Often mispronounced\n\nWHY NOT \"LUMO-AI\"?\n  \u2717 Too long (multiple syllables = complex model)\n  \u2717 Hyphen creates ambiguity in speech\n  \u2717 Users will just say \"Lumo\" anyway\n\nDECISION: \"Lumo\" is optimal (short, unique, memorable)\n\n=== HOW KEYWORD SPOTTING WORKS ===\n\nMETHOD 1: ACOUSTIC MATCHING (Old approach)\n  Pre-record wake word: \"Lumo... Lumo... Lumo...\" (many times)\n  Compare incoming audio to recorded samples\n  Problem: Doesn't handle differ"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:3", "text": " recorded samples\n  Problem: Doesn't handle different accents, pitches, speeds\n  Result: High false rejection rate\n\nMETHOD 2: MACHINE LEARNING (Modern approach)\n  Train classifier on thousands of \"Lumo\" and \"not Lumo\" samples\n  Model learns acoustic patterns\n  Can handle variations: accent, pitch, speed, background noise\n  Result: High accuracy, low false positives\n\nMETHOD 3: HYBRID (WebRTC VAD + Model)\n  Step 1: Voice Activity Detection (VAD) detects speech\n  Step 2: Extract audio when speech d"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:4", "text": "tects speech\n  Step 2: Extract audio when speech detected\n  Step 3: Run ML classifier on extracted segment\n  Step 4: If >threshold confidence: trigger\n  Benefits: Reduces false positives, saves CPU\n\nLUMO USES: METHOD 3 (Hybrid VAD + Model)\n\n=== VAD (VOICE ACTIVITY DETECTION) ===\n\nWHAT IS VAD?\n  Detects whether audio contains speech or silence\n  Filters out background noise, music, silence\n  Used to reduce CPU and false positives\n\nWHY IMPORTANT FOR WAKE WORD?\n  Without VAD: Listening to every sou"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:5", "text": "R WAKE WORD?\n  Without VAD: Listening to every sound (expensive, inaccurate)\n  With VAD: Only listen when speech detected (efficient, accurate)\n  Result: Lower CPU usage, fewer false positives\n\nLUMO IMPLEMENTATION:\n  Uses WebRTC VAD (open-source, efficient)\n  Runs on CPU (no GPU needed)\n  Detects speech with ~95% accuracy\n  Falls back to full audio processing only when speech detected\n\n=== WAKE WORD DETECTION FLOW ===\n\nFLOW DIAGRAM:\n  Audio Stream\n    \u2193 (every 10ms chunk)\n  WebRTC VAD: Is this s"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:6", "text": "m\n    \u2193 (every 10ms chunk)\n  WebRTC VAD: Is this speech?\n    \u251c\u2500 NO: Discard chunk, continue listening\n    \u2514\u2500 YES: Extract audio segment\n         \u2193\n       Transcribe with Faster-Whisper\n         \u2193\n       Check: Does text contain \"lumo\"?\n         \u251c\u2500 NO: Ignore\n         \u2514\u2500 YES: WAKE WORD DETECTED\n              \u2193\n            Start recording user command\n              \u2193\n            Process with LLM\n\n=== WAKE WORD ACCURACY METRICS ===\n\nFALSE POSITIVES:\n  \"Lumo\" said in TV show \u2192 System triggers (bad)\n"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:7", "text": "\n  \"Lumo\" said in TV show \u2192 System triggers (bad)\n  Rate: Goal <1% (1 false trigger per 100 hours of audio)\n  Mitigation: Train model on negative examples\n\nFALSE NEGATIVES:\n  User says \"Lumo\" \u2192 System doesn't trigger (bad)\n  Rate: Goal <5% (95% detection accuracy)\n  Mitigation: Use robust VAD + strong model\n\nTRADEOFF:\n  Lower threshold: More false positives (annoying)\n  Higher threshold: More false negatives (frustrating)\n  Solution: Find sweet spot (95% accuracy, <1% false positives)\n\n=== WHY N"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:8", "text": "pot (95% accuracy, <1% false positives)\n\n=== WHY NOT ALWAYS LISTEN (WITHOUT WAKE WORD)? ===\n\nOPTION A: NO WAKE WORD (Process all audio)\n  Pros:\n    - No missed commands (100% detection)\n    - Faster response (no delay for wake word)\n  Cons:\n    - High CPU usage (processes constant audio)\n    - Privacy concerns (always recording)\n    - False triggers on similar-sounding words\n    - Processes background noise, TV, other people\n  Result: Not practical\n\nOPTION B: WAKE WORD (Current approach)\n  Pros:"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:9", "text": "al\n\nOPTION B: WAKE WORD (Current approach)\n  Pros:\n    - Low CPU usage (only process when triggered)\n    - Better privacy (doesn't always listen)\n    - Few false triggers (selective)\n    - Efficient\n  Cons:\n    - Could miss some commands (if wake word not clear)\n    - Slight delay (need to say wake word first)\n  Result: Best tradeoff\n\nDECISION: WAKE WORD is necessary for practical voice interaction\n\n=== WAKE WORD ALTERNATIVES ===\n\nOPTION 1: MULTIPLE WAKE WORDS\n  \"Lumo\", \"Hey Lumo\", \"Computer\", \""}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:10", "text": "PLE WAKE WORDS\n  \"Lumo\", \"Hey Lumo\", \"Computer\", \"Assistant\"\n  Pro: More flexible\n  Con: Complex model, higher false positives\n  Decision: Stick with single \"Lumo\"\n\nOPTION 2: VOICE AUTHENTICATION\n  Only respond to owner's voice\n  Pro: Highest security\n  Con: Too complex, doesn't match use case\n  Decision: Not needed for single-user system\n\nOPTION 3: SPEAKER DIARIZATION\n  Detect when user is addressing you (via context)\n  Pro: Most natural\n  Con: Requires ML model + LLM inference (too slow)\n  Dec"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:11", "text": "Requires ML model + LLM inference (too slow)\n  Decision: Impractical for real-time voice\n\n=== LEARNING FROM WAKE WORD MISSES ===\n\nCURRENT: Static wake word detector\nFUTURE: Learn from misses\n\nHOW TO IMPROVE:\n  1. Log every false negative: \"User said Lumo, system didn't trigger\"\n  2. Log confidence scores: \"Model was 40% confident (below 60% threshold)\"\n  3. Collect data: Gather more examples of \"Lumo\" with low confidence\n  4. Retrain: Fine-tune model on problematic examples\n  5. Deploy: Update d"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:12", "text": "odel on problematic examples\n  5. Deploy: Update detector with improved model\n\nRESULT: Wake word accuracy improves over time\n\n=== IMPLEMENTATION DETAILS ===\n\nCURRENT IMPLEMENTATION (audio/stt.py):\n  1. Capture audio in 16kHz chunks\n  2. Run WebRTC VAD (detects speech)\n  3. When speech detected: transcribe segment\n  4. Check if \"lumo\" in transcription (case-insensitive)\n  5. If found: system is triggered, ready for command\n  6. If not found: continue listening\n\nCONFIDENCE METRICS:\n  - VAD confide"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:13", "text": "nue listening\n\nCONFIDENCE METRICS:\n  - VAD confidence: 0-1 (is this speech?)\n  - STT confidence: Not available (Faster-Whisper doesn't provide)\n  - Wake word matching: Binary (found or not found)\n\nIMPROVEMENTS NEEDED:\n  1. Add confidence score to STT\n  2. Log false negatives for analysis\n  3. Implement threshold tuning based on performance\n\n=== PRIVACY IMPLICATIONS ===\n\nWITH WAKE WORD:\n  \u2713 Device only processes audio after \"Lumo\" detected\n  \u2713 Background noise ignored\n  \u2713 Other people's conversat"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:14", "text": "kground noise ignored\n  \u2713 Other people's conversations ignored\n  \u2713 Privacy-preserving (mostly local)\n\nWITHOUT WAKE WORD:\n  \u2717 Every sound processed (privacy concern)\n  \u2717 Constant transcription (energy intensive)\n  \u2717 Could capture sensitive info (accidental)\n\nTRUST:\n  Wake word = explicit signal that device should listen\n  User says \"Lumo\" = clear consent to process audio\n  Respects user autonomy and privacy\n\n=== FUTURE IMPROVEMENTS ===\n\nIMPROVEMENT 1: ON-DEVICE WAKE WORD MODEL\n  Current: Uses tex"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:15", "text": "T 1: ON-DEVICE WAKE WORD MODEL\n  Current: Uses text-based detection (transcribe then match)\n  Future: Use audio-based model (detect \"Lumo\" sound without transcription)\n  Benefit: Even lower latency, lower CPU, better privacy\n\nIMPROVEMENT 2: CUSTOM WAKE WORDS\n  Current: Fixed \"Lumo\"\n  Future: User can set custom wake word\n  Benefit: Personalization, flexibility\n\nIMPROVEMENT 3: WAKE WORD + SPEAKER VERIFICATION\n  Current: Any voice saying \"Lumo\" triggers\n  Future: Only owner's voice triggers\n  Bene"}
{"source": "WHY_KEYWORD_SPOTTING.txt", "id": "WHY_KEYWORD_SPOTTING.txt:16", "text": "ggers\n  Future: Only owner's voice triggers\n  Benefit: Security, multi-user scenarios\n\nIMPROVEMENT 4: CONTEXT-AWARE WAKE WORD\n  Current: \"Lumo\" anywhere triggers\n  Future: Detect if user is addressing you (via context)\n  Benefit: More natural interaction\n"}
